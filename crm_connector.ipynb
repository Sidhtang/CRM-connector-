{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZQHtv+HPYLEFllPV2hH0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/CRM-connector-/blob/main/crm_connector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DhqHGmk_qAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import json\n",
        "import requests\n",
        "import redis\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "import xmlrpc.client  # For Odoo XML-RPC API\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('crm_integration.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Enhanced Data Structures\n",
        "@dataclass\n",
        "class UserInput:\n",
        "    \"\"\"Stores parsed user input and categorization\"\"\"\n",
        "    raw_input: str\n",
        "    category: str\n",
        "    details: Dict[str, Any]\n",
        "    timestamp: datetime\n",
        "\n",
        "@dataclass\n",
        "class IntentContext:\n",
        "    \"\"\"Stores parsed user intent and relevant entities with additional metadata\"\"\"\n",
        "    raw_input: str\n",
        "    intent: str\n",
        "    entities: Dict[str, Any]\n",
        "    actions: List[str]\n",
        "    confidence: float\n",
        "    timestamp: datetime\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "@dataclass\n",
        "class WorkflowResult:\n",
        "    \"\"\"Stores the result of workflow execution with detailed metrics\"\"\"\n",
        "    success: bool\n",
        "    messages: List[str]\n",
        "    data: Dict[str, Any]\n",
        "    execution_time: float\n",
        "    resource_usage: Dict[str, float] = None\n",
        "    error_details: Optional[Dict[str, Any]] = None\n",
        "\n",
        "# Simple Secret Manager Implementation\n",
        "class SimpleSecretManager:\n",
        "    def __init__(self, config_file: str = \"secrets.json\"):\n",
        "        self.config_file = config_file\n",
        "        try:\n",
        "            with open(config_file, 'r') as f:\n",
        "                self.secrets = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            self.secrets = {}\n",
        "            with open(config_file, 'w') as f:\n",
        "                json.dump(self.secrets, f)\n",
        "\n",
        "    def get_secret(self, path: str, key: str) -> str:\n",
        "        return self.secrets.get(path, {}).get(key)\n",
        "\n",
        "    def store_secret(self, path: str, key: str, value: str) -> bool:\n",
        "        try:\n",
        "            if path not in self.secrets:\n",
        "                self.secrets[path] = {}\n",
        "            self.secrets[path][key] = value\n",
        "            with open(self.config_file, 'w') as f:\n",
        "                json.dump(self.secrets, f)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to store secret: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "# Metrics Collection\n",
        "class MetricsCollector:\n",
        "    def __init__(self):\n",
        "        self.redis_client = redis.Redis()\n",
        "\n",
        "    def record_metric(self, metric_name: str, value: float, tags: Dict[str, str]):\n",
        "        timestamp = datetime.now().timestamp()\n",
        "        key = f\"metrics:{metric_name}:{timestamp}\"\n",
        "        self.redis_client.hset(key, mapping={\n",
        "            \"value\": value,\n",
        "            \"timestamp\": timestamp,\n",
        "            **tags\n",
        "        })\n",
        "\n",
        "    def get_metrics(self, metric_name: str, time_range: Tuple[datetime, datetime]) -> pd.DataFrame:\n",
        "        keys = self.redis_client.keys(f\"metrics:{metric_name}:*\")\n",
        "        data = []\n",
        "        for key in keys:\n",
        "            metric_data = self.redis_client.hgetall(key)\n",
        "            if time_range[0].timestamp() <= float(metric_data[b\"timestamp\"]) <= time_range[1].timestamp():\n",
        "                data.append(metric_data)\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "# CRM Connector Implementations\n",
        "class BaseCRMConnector:\n",
        "    def __init__(self, credentials: Dict[str, str]):\n",
        "        self.credentials = credentials\n",
        "        self.metrics = MetricsCollector()\n",
        "\n",
        "    def authenticate(self) -> bool:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def fetch_data(self, query: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def push_data(self, data: List[Dict[str, Any]]) -> bool:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class OdooConnector(BaseCRMConnector):\n",
        "    def authenticate(self) -> bool:\n",
        "        try:\n",
        "            # Implement Odoo XML-RPC authentication\n",
        "            self.url = self.credentials.get(\"url\", \"http://localhost:8069\")\n",
        "            self.db = self.credentials.get(\"database\", \"odoo\")\n",
        "            self.username = self.credentials.get(\"username\", \"admin\")\n",
        "            self.password = self.credentials.get(\"password\", \"admin\")\n",
        "\n",
        "            # Common endpoint for authentication\n",
        "            common = xmlrpc.client.ServerProxy(f'{self.url}/xmlrpc/2/common')\n",
        "            self.uid = common.authenticate(self.db, self.username, self.password, {})\n",
        "\n",
        "            if self.uid:\n",
        "                # Initialize models endpoint for data operations\n",
        "                self.models = xmlrpc.client.ServerProxy(f'{self.url}/xmlrpc/2/object')\n",
        "                return True\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Odoo authentication failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def fetch_data(self, query: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        if not hasattr(self, \"uid\"):\n",
        "            raise ValueError(\"Not authenticated\")\n",
        "\n",
        "        # Implement Odoo search_read operation\n",
        "        try:\n",
        "            model = query.get(\"object_type\", \"res.partner\")\n",
        "            fields = query.get(\"fields\", [\"id\", \"name\", \"email\", \"phone\"])\n",
        "            domain = self._build_odoo_domain(query.get(\"where\", []))\n",
        "            limit = query.get(\"limit\", 100)\n",
        "\n",
        "            records = self.models.execute_kw(\n",
        "                self.db, self.uid, self.password,\n",
        "                model, 'search_read',\n",
        "                [domain], {\n",
        "                    'fields': fields,\n",
        "                    'limit': limit\n",
        "                }\n",
        "            )\n",
        "\n",
        "            return records\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching data from Odoo: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def push_data(self, data: List[Dict[str, Any]]) -> bool:\n",
        "        if not hasattr(self, \"uid\"):\n",
        "            raise ValueError(\"Not authenticated\")\n",
        "\n",
        "        # Implement data push to Odoo\n",
        "        try:\n",
        "            # Determine object type from first record\n",
        "            if not data:\n",
        "                return True  # No data to push\n",
        "\n",
        "            success_count = 0\n",
        "            for record in data:\n",
        "                model = record.pop(\"object_type\", \"res.partner\")\n",
        "\n",
        "                # Check if record has ID (update) or not (create)\n",
        "                record_id = record.pop(\"id\", None)\n",
        "\n",
        "                if record_id:\n",
        "                    # Update existing record\n",
        "                    result = self.models.execute_kw(\n",
        "                        self.db, self.uid, self.password,\n",
        "                        model, 'write',\n",
        "                        [[record_id], record]\n",
        "                    )\n",
        "                    if result:\n",
        "                        success_count += 1\n",
        "                else:\n",
        "                    # Create new record\n",
        "                    record_id = self.models.execute_kw(\n",
        "                        self.db, self.uid, self.password,\n",
        "                        model, 'create',\n",
        "                        [record]\n",
        "                    )\n",
        "                    if record_id:\n",
        "                        success_count += 1\n",
        "\n",
        "            return success_count == len(data)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error pushing data to Odoo: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def _build_odoo_domain(self, filters: List) -> List:\n",
        "        # Convert query parameters to Odoo domain format\n",
        "        if not filters:\n",
        "            return []\n",
        "\n",
        "        # If filters is already in Odoo domain format, return as is\n",
        "        if isinstance(filters, list) and all(isinstance(item, tuple) or isinstance(item, list) for item in filters):\n",
        "            return filters\n",
        "\n",
        "        # Simple conversion from dict-based filters to Odoo domain\n",
        "        domain = []\n",
        "        for field, value in filters.items():\n",
        "            if isinstance(value, dict):\n",
        "                operator = value.get('operator', '=')\n",
        "                val = value.get('value')\n",
        "                domain.append((field, operator, val))\n",
        "            else:\n",
        "                domain.append((field, '=', value))\n",
        "\n",
        "        return domain\n",
        "\n",
        "class ZohoCRMConnector(BaseCRMConnector):\n",
        "    def authenticate(self) -> bool:\n",
        "        try:\n",
        "            # Implement Zoho OAuth2 authentication\n",
        "            self.client_id = self.credentials.get(\"client_id\")\n",
        "            self.client_secret = self.credentials.get(\"client_secret\")\n",
        "            self.refresh_token = self.credentials.get(\"refresh_token\")\n",
        "            self.domain = self.credentials.get(\"domain\", \"com\")  # Zoho domain (com, eu, in, etc.)\n",
        "\n",
        "            # Zoho API endpoints\n",
        "            self.base_url = f\"https://www.zohoapis.{self.domain}/crm/v2\"\n",
        "            self.auth_url = f\"https://accounts.zoho.{self.domain}/oauth/v2/token\"\n",
        "\n",
        "            # Get access token using refresh token\n",
        "            auth_data = {\n",
        "                \"refresh_token\": self.refresh_token,\n",
        "                \"client_id\": self.client_id,\n",
        "                \"client_secret\": self.client_secret,\n",
        "                \"grant_type\": \"refresh_token\"\n",
        "            }\n",
        "\n",
        "            response = requests.post(self.auth_url, data=auth_data)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                token_data = response.json()\n",
        "                self.access_token = token_data.get(\"access_token\")\n",
        "                self.token_expiry = datetime.now().timestamp() + token_data.get(\"expires_in\", 3600)\n",
        "                self.headers = {\n",
        "                    \"Authorization\": f\"Zoho-oauthtoken {self.access_token}\",\n",
        "                    \"Content-Type\": \"application/json\"\n",
        "                }\n",
        "                return True\n",
        "            else:\n",
        "                logger.error(f\"Zoho authentication failed: {response.text}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Zoho authentication failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def fetch_data(self, query: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Fetch data from Zoho CRM using their REST API v2.\n",
        "        Supports pagination and field selection.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"access_token\"):\n",
        "            raise ValueError(\"Not authenticated\")\n",
        "\n",
        "        # Check token expiry and refresh if needed\n",
        "        self._check_token_expiry()\n",
        "\n",
        "        try:\n",
        "            module = query.get(\"object_type\", \"Leads\")\n",
        "            fields = query.get(\"fields\", [\"id\", \"Full_Name\", \"Email\", \"Phone\"])\n",
        "            where_clause = query.get(\"where\", \"\")\n",
        "            limit = query.get(\"limit\", 100)\n",
        "\n",
        "            # Construct API parameters\n",
        "            params = {\n",
        "                \"fields\": \",\".join(fields),\n",
        "                \"per_page\": min(limit, 200)  # Zoho's max per page is 200\n",
        "            }\n",
        "\n",
        "            # Add search criteria if provided\n",
        "            if where_clause:\n",
        "                if isinstance(where_clause, dict):\n",
        "                    # Convert dict filters to Zoho criteria format\n",
        "                    criteria = []\n",
        "                    for field, value in where_clause.items():\n",
        "                        if isinstance(value, dict):\n",
        "                            operator = self._map_operator(value.get('operator', '='))\n",
        "                            val = value.get('value')\n",
        "                            criteria.append(f\"({field}:{operator}:{val})\")\n",
        "                        else:\n",
        "                            criteria.append(f\"({field}:equals:{value})\")\n",
        "                    params[\"criteria\"] = \"(\" + \" and \".join(criteria) + \")\"\n",
        "                else:\n",
        "                    params[\"criteria\"] = where_clause\n",
        "\n",
        "            all_records = []\n",
        "            page = 1\n",
        "\n",
        "            while True:\n",
        "                params[\"page\"] = page\n",
        "                url = f\"{self.base_url}/{module}/search\"\n",
        "\n",
        "                response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    records = data.get(\"data\", [])\n",
        "                    all_records.extend(records)\n",
        "\n",
        "                    # Check if we have more pages\n",
        "                    info = data.get(\"info\", {})\n",
        "                    if info.get(\"more_records\", False) and len(all_records) < limit:\n",
        "                        page += 1\n",
        "                    else:\n",
        "                        break\n",
        "                else:\n",
        "                    logger.error(f\"Failed to fetch data from Zoho: {response.text}\")\n",
        "                    raise Exception(f\"Failed to fetch data: {response.text}\")\n",
        "\n",
        "            return all_records[:limit]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching data from Zoho: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def push_data(self, data: List[Dict[str, Any]]) -> bool:\n",
        "        \"\"\"\n",
        "        Push data to Zoho CRM using their REST API v2.\n",
        "        Supports both creating new records and updating existing ones.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"access_token\"):\n",
        "            raise ValueError(\"Not authenticated\")\n",
        "\n",
        "        # Check token expiry and refresh if needed\n",
        "        self._check_token_expiry()\n",
        "\n",
        "        try:\n",
        "            if not data:\n",
        "                return True  # No data to push\n",
        "\n",
        "            success_count = 0\n",
        "            batch_size = 100  # Zoho allows up to 100 records per request\n",
        "\n",
        "            # Process records in batches\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                batch = data[i:i + batch_size]\n",
        "\n",
        "                # Separate records for update and insert\n",
        "                updates = []\n",
        "                inserts = []\n",
        "\n",
        "                for record in batch:\n",
        "                    module = record.pop(\"object_type\", \"Leads\")\n",
        "                    record_id = record.pop(\"id\", None)\n",
        "\n",
        "                    if record_id:\n",
        "                        record[\"id\"] = record_id\n",
        "                        updates.append(record)\n",
        "                    else:\n",
        "                        inserts.append(record)\n",
        "\n",
        "                # Handle inserts\n",
        "                if inserts:\n",
        "                    url = f\"{self.base_url}/{module}\"\n",
        "                    response = requests.post(\n",
        "                        url,\n",
        "                        headers=self.headers,\n",
        "                        json={\"data\": inserts}\n",
        "                    )\n",
        "\n",
        "                    if response.status_code in (200, 201):\n",
        "                        success_count += len(inserts)\n",
        "                    else:\n",
        "                        logger.error(f\"Failed to insert records in Zoho: {response.text}\")\n",
        "\n",
        "                # Handle updates\n",
        "                if updates:\n",
        "                    url = f\"{self.base_url}/{module}\"\n",
        "                    response = requests.put(\n",
        "                        url,\n",
        "                        headers=self.headers,\n",
        "                        json={\"data\": updates}\n",
        "                    )\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        success_count += len(updates)\n",
        "                    else:\n",
        "                        logger.error(f\"Failed to update records in Zoho: {response.text}\")\n",
        "\n",
        "            return success_count == len(data)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error pushing data to Zoho: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def _check_token_expiry(self):\n",
        "        \"\"\"Check if token is expired and refresh if needed\"\"\"\n",
        "        if datetime.now().timestamp() >= self.token_expiry:\n",
        "            self.authenticate()\n",
        "\n",
        "    def _map_operator(self, operator: str) -> str:\n",
        "        \"\"\"Map common operators to Zoho's operator format\"\"\"\n",
        "        operator_map = {\n",
        "            '=': 'equals',\n",
        "            '!=': 'not_equals',\n",
        "            '>': 'greater_than',\n",
        "            '<': 'less_than',\n",
        "            '>=': 'greater_equals',\n",
        "            '<=': 'less_equals',\n",
        "            'like': 'contains',\n",
        "            'not like': 'not_contains',\n",
        "            'in': 'in',\n",
        "            'not in': 'not_in',\n",
        "            'starts with': 'starts_with',\n",
        "            'ends with': 'ends_with'\n",
        "        }\n",
        "        return operator_map.get(operator, 'equals')\n",
        "\n",
        "# Enhanced Schema Mapping\n",
        "class SchemaMapper:\n",
        "    def __init__(self):\n",
        "        self.metrics = MetricsCollector()\n",
        "\n",
        "    def map_schema(self, source_data: Dict, source_crm: str, target_crm: str) -> Dict:\n",
        "        mapping_start = datetime.now()\n",
        "\n",
        "        # Load schema mapping rules\n",
        "        mapping_rules = self._load_mapping_rules(source_crm, target_crm)\n",
        "\n",
        "        # Apply transformations\n",
        "        mapped_data = self._apply_mapping_rules(source_data, mapping_rules)\n",
        "\n",
        "        # Validate mapped data\n",
        "        self._validate_mapped_data(mapped_data, target_crm)\n",
        "\n",
        "        # Record metrics\n",
        "        mapping_time = (datetime.now() - mapping_start).total_seconds()\n",
        "        self.metrics.record_metric(\n",
        "            \"schema_mapping_time\",\n",
        "            mapping_time,\n",
        "            {\n",
        "                \"source_crm\": source_crm,\n",
        "                \"target_crm\": target_crm\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return mapped_data\n",
        "\n",
        "    def _load_mapping_rules(self, source_crm: str, target_crm: str) -> Dict:\n",
        "        # Load and parse mapping rules from configuration\n",
        "        with open(f\"mapping_rules/{source_crm}_to_{target_crm}.json\", \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _apply_mapping_rules(self, source_data: Dict, mapping_rules: Dict) -> Dict:\n",
        "        mapped_data = {}\n",
        "        for target_field, rule in mapping_rules.items():\n",
        "            if rule[\"type\"] == \"direct\":\n",
        "                mapped_data[target_field] = source_data.get(rule[\"source_field\"])\n",
        "            elif rule[\"type\"] == \"transform\":\n",
        "                mapped_data[target_field] = self._apply_transformation(\n",
        "                    source_data,\n",
        "                    rule[\"transformation\"]\n",
        "                )\n",
        "        return mapped_data\n",
        "\n",
        "    def _apply_transformation(self, source_data: Dict, transformation_info: Dict) -> Any:\n",
        "        # Apply transformations based on transformation type\n",
        "        transform_type = transformation_info.get(\"type\")\n",
        "\n",
        "        if transform_type == \"concat\":\n",
        "            fields = transformation_info.get(\"fields\", [])\n",
        "            separator = transformation_info.get(\"separator\", \" \")\n",
        "            values = [str(source_data.get(field, \"\")) for field in fields]\n",
        "            return separator.join(values)\n",
        "\n",
        "        elif transform_type == \"split\":\n",
        "            field = transformation_info.get(\"field\")\n",
        "            separator = transformation_info.get(\"separator\", \" \")\n",
        "            index = transformation_info.get(\"index\", 0)\n",
        "            if field in source_data:\n",
        "                parts = source_data[field].split(separator)\n",
        "                if 0 <= index < len(parts):\n",
        "                    return parts[index]\n",
        "            return None\n",
        "\n",
        "        elif transform_type == \"date_format\":\n",
        "            field = transformation_info.get(\"field\")\n",
        "            source_format = transformation_info.get(\"source_format\")\n",
        "            target_format = transformation_info.get(\"target_format\")\n",
        "            if field in source_data:\n",
        "                try:\n",
        "                    date_obj = datetime.strptime(source_data[field], source_format)\n",
        "                    return date_obj.strftime(target_format)\n",
        "                except ValueError:\n",
        "                    return source_data[field]\n",
        "            return None\n",
        "\n",
        "        # Add more transformation types as needed\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _validate_mapped_data(self, mapped_data: Dict, target_crm: str):\n",
        "        # Implement validation logic for target CRM schema\n",
        "        pass\n",
        "\n",
        "# Workflow Management with Error Handling\n",
        "class WorkflowManager:\n",
        "    def __init__(self):\n",
        "        self.metrics_collector = MetricsCollector()\n",
        "        self.input_handler = InputTypeHandler()\n",
        "        self.schema_mapper = SchemaMapper()\n",
        "\n",
        "    def process_workflow(self, user_input: str) -> WorkflowResult:\n",
        "        start_time = datetime.now()\n",
        "        resource_usage = {\n",
        "            \"memory\": 0,\n",
        "            \"cpu\": 0,\n",
        "            \"api_calls\": 0\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Categorize input\n",
        "            input_data = self.input_handler.categorize_input(user_input)\n",
        "\n",
        "            # Execute appropriate workflow\n",
        "            if input_data.category == \"data_sync\":\n",
        "                result = self._handle_data_sync(input_data)\n",
        "                resource_usage[\"api_calls\"] += 2  # Authentication + data fetch\n",
        "            elif input_data.category == \"data_query\":\n",
        "                result = self._handle_data_query(input_data)\n",
        "                resource_usage[\"api_calls\"] += 1  # Data query\n",
        "            elif input_data.category == \"data_update\":\n",
        "                result = self._handle_data_update(input_data)\n",
        "                resource_usage[\"api_calls\"] += 1  # Data update\n",
        "            elif input_data.category == \"scheduling\":\n",
        "                result = self._handle_scheduling(input_data)\n",
        "            elif input_data.category == \"reporting\":\n",
        "                result = self._handle_reporting(input_data)\n",
        "                resource_usage[\"memory\"] += 100  # Report generation\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown input category: {input_data.category}\")\n",
        "\n",
        "            execution_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "            return WorkflowResult(\n",
        "                success=True,\n",
        "                messages=result.get(\"messages\", []),\n",
        "                data=result.get(\"data\", {}),\n",
        "                execution_time=execution_time,\n",
        "                resource_usage=resource_usage\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Workflow execution failed: {str(e)}\", exc_info=True)\n",
        "            execution_time = (datetime.now() - start_time).total_seconds()\n",
        "            return WorkflowResult(\n",
        "                success=False,\n",
        "                messages=[f\"Error: {str(e)}\"],\n",
        "                data={},\n",
        "                execution_time=execution_time,\n",
        "                resource_usage=resource_usage,\n",
        "                error_details={\"error_type\": type(e).__name__, \"error_message\": str(e)}\n",
        "            )\n",
        "\n",
        "    def _handle_data_sync(self, input_data: UserInput) -> Dict[str, Any]:\n",
        "        # Implement data synchronization workflow\n",
        "        source_system = input_data.details.get(\"source\", \"unknown\")\n",
        "        target_system = input_data.details.get(\"target\", \"unknown\")\n",
        "\n",
        "        # Set up source and target CRM connectors\n",
        "        source_connector = self._get_connector(source_system)\n",
        "        target_connector = self._get_connector(target_system)\n",
        "\n",
        "        # Authenticate with both systems\n",
        "        source_authenticated = source_connector.authenticate()\n",
        "        target_authenticated = target_connector.authenticate()\n",
        "\n",
        "        if not (source_authenticated and target_authenticated):\n",
        "            raise Exception(f\"Failed to authenticate with {source_system} or {target_system}\")\n",
        "\n",
        "        # Fetch data from source\n",
        "        source_data = source_connector.fetch_data({\"object_type\": \"Lead\", \"limit\": 100})\n",
        "\n",
        "        # Map schema between systems\n",
        "        mapped_data = []\n",
        "        for record in source_data:\n",
        "            mapped_record = self.schema_mapper.map_schema(record, source_system, target_system)\n",
        "            mapped_data.append(mapped_record)\n",
        "\n",
        "        # Push to target\n",
        "        success = target_connector.push_data(mapped_data)\n",
        "\n",
        "        return {\n",
        "            \"messages\": [f\"Syncing data from {source_system} to {target_system}\",\n",
        "                         f\"Synced {len(mapped_data)} records successfully\" if success else \"Sync failed\"],\n",
        "            \"data\": {\n",
        "                \"sync_status\": \"completed\" if success else \"failed\",\n",
        "                \"records_processed\": len(source_data),\n",
        "                \"records_synced\": len(mapped_data) if success else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _handle_data_query(self, input_data: UserInput) -> Dict[str, Any]:\n",
        "        # Implement data query workflow\n",
        "        query_params = input_data.details.get(\"query_params\", {})\n",
        "        source_system = input_data.details.get(\"source\", \"unknown\")\n",
        "\n",
        "        # Set up connector\n",
        "        connector = self._get_connector(source_system)\n",
        "\n",
        "        # Authenticate\n",
        "        if not connector.authenticate():\n",
        "            raise Exception(f\"Failed to authenticate with {source_system}\")\n",
        "\n",
        "        # Execute query\n",
        "        results = connector.fetch_data(query_params)\n",
        "\n",
        "        return {\n",
        "            \"messages\": [f\"Executed query against {source_system}\",\n",
        "                         f\"Found {len(results)} matching records\"],\n",
        "            \"data\": {\"query_results\": results}\n",
        "        }\n",
        "\n",
        "    def _handle_data_update(self, input_data: UserInput) -> Dict[str, Any]:\n",
        "        # Implement data update workflow\n",
        "        target_system = input_data.details.get(\"target\", \"unknown\")\n",
        "        records = input_data.details.get(\"records\", [])\n",
        "\n",
        "        if not records:\n",
        "            return {\n",
        "                \"messages\": [\"No records to update\"],\n",
        "                \"data\": {\"update_status\": \"skipped\"}\n",
        "            }\n",
        "\n",
        "        # Set up connector\n",
        "        connector = self._get_connector(target_system)\n",
        "\n",
        "        # Authenticate\n",
        "        if not connector.authenticate():\n",
        "            raise Exception(f\"Failed to authenticate with {target_system}\")\n",
        "\n",
        "        # Push updates\n",
        "        success = connector.push_data(records)\n",
        "\n",
        "        return {\n",
        "            \"messages\": [f\"Updating records in {target_system}\",\n",
        "                        f\"Updated {len(records)} records successfully\" if success else \"Update failed\"],\n",
        "            \"data\": {\n",
        "                \"update_status\": \"completed\" if success else \"failed\",\n",
        "                \"records_updated\": len(records) if success else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _handle_scheduling(self, input_data: UserInput) -> Dict[str, Any]:\n",
        "        # Implement scheduling workflow\n",
        "        schedule_details = input_data.details.get(\"schedule\", {})\n",
        "        target_system = input_data.details.get(\"target\", \"unknown\")\n",
        "\n",
        "        # Validate schedule details\n",
        "        if not schedule_details.get(\"task\"):\n",
        "            raise ValueError(\"No task specified for scheduling\")\n",
        "\n",
        "        # Set up connector for validation\n",
        "        connector = self._get_connector(target_system)\n",
        "\n",
        "        # Authenticate to validate target system\n",
        "        if not connector.authenticate():\n",
        "            raise Exception(f\"Failed to authenticate with {target_system}\")\n",
        "\n",
        "        # Add schedule to database (mock implementation)\n",
        "        schedule_id = self._create_schedule_entry(schedule_details, target_system)\n",
        "\n",
        "        return {\n",
        "            \"messages\": [f\"Scheduled task '{schedule_details.get('task')}' for {target_system}\",\n",
        "                        f\"Schedule ID: {schedule_id}\"],\n",
        "            \"data\": {\n",
        "                \"schedule_id\": schedule_id,\n",
        "                \"schedule_details\": schedule_details\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _handle_reporting(self, input_data: UserInput) -> Dict[str, Any]:\n",
        "        # Implement reporting workflow\n",
        "        report_type = input_data.details.get(\"report_type\", \"summary\")\n",
        "        source_system = input_data.details.get(\"source\", \"unknown\")\n",
        "        date_range = input_data.details.get(\"date_range\", {\"days\": 30})\n",
        "\n",
        "        # Set up connector\n",
        "        connector = self._get_connector(source_system)\n",
        "\n",
        "        # Authenticate\n",
        "        if not connector.authenticate():\n",
        "            raise Exception(f\"Failed to authenticate with {source_system}\")\n",
        "\n",
        "        # Fetch data for report\n",
        "        query = self._build_report_query(report_type, date_range)\n",
        "        data = connector.fetch_data(query)\n",
        "\n",
        "        # Generate report\n",
        "        report = self._generate_report(data, report_type)\n",
        "\n",
        "        return {\n",
        "            \"messages\": [f\"Generated {report_type} report for {source_system}\",\n",
        "                        f\"Report covers data from the last {date_range.get('days')} days\"],\n",
        "            \"data\": {\n",
        "                \"report\": report,\n",
        "                \"metadata\": {\n",
        "                    \"report_type\": report_type,\n",
        "                    \"date_range\": date_range,\n",
        "                    \"record_count\": len(data)\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _get_connector(self, system_name: str) -> BaseCRMConnector:\n",
        "        \"\"\"Create and return appropriate connector based on system name\"\"\"\n",
        "        # Get credentials from secret manager\n",
        "        secret_manager = SimpleSecretManager()\n",
        "        credentials = {\n",
        "            \"url\": secret_manager.get_secret(system_name, \"url\"),\n",
        "            \"username\": secret_manager.get_secret(system_name, \"username\"),\n",
        "            \"password\": secret_manager.get_secret(system_name, \"password\"),\n",
        "            \"database\": secret_manager.get_secret(system_name, \"database\"),\n",
        "            \"client_id\": secret_manager.get_secret(system_name, \"client_id\"),\n",
        "            \"client_secret\": secret_manager.get_secret(system_name, \"client_secret\"),\n",
        "            \"refresh_token\": secret_manager.get_secret(system_name, \"refresh_token\"),\n",
        "            \"domain\": secret_manager.get_secret(system_name, \"domain\")\n",
        "        }\n",
        "\n",
        "        # Create appropriate connector\n",
        "        if system_name.lower() == \"odoo\":\n",
        "            return OdooConnector(credentials)\n",
        "        elif system_name.lower() == \"zoho\":\n",
        "            return ZohoCRMConnector(credentials)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown CRM system: {system_name}\")\n",
        "\n",
        "    def _create_schedule_entry(self, schedule_details: Dict, target_system: str) -> str:\n",
        "        \"\"\"Create a schedule entry in the database and return schedule ID\"\"\"\n",
        "        # Mock implementation - in real system, this would create an entry in a database\n",
        "        schedule_id = f\"SCH-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
        "        # Here you would save the schedule to a database\n",
        "        return schedule_id\n",
        "\n",
        "    def _build_report_query(self, report_type: str, date_range: Dict) -> Dict:\n",
        "        \"\"\"Build query parameters based on report type and date range\"\"\"\n",
        "        query = {\"limit\": 1000}\n",
        "\n",
        "        # Set appropriate object type based on report type\n",
        "        if report_type == \"leads\":\n",
        "            query[\"object_type\"] = \"Lead\"\n",
        "        elif report_type == \"opportunities\":\n",
        "            query[\"object_type\"] = \"Opportunity\"\n",
        "        elif report_type == \"contacts\":\n",
        "            query[\"object_type\"] = \"Contact\"\n",
        "        else:\n",
        "            query[\"object_type\"] = \"Lead\"  # Default to leads for summary reports\n",
        "\n",
        "        # Add date range filter\n",
        "        days = date_range.get(\"days\", 30)\n",
        "        start_date = (datetime.now() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
        "        query[\"where\"] = {\n",
        "            \"date_created\": {\n",
        "                \"operator\": \">=\",\n",
        "                \"value\": start_date\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Add fields based on report type\n",
        "        if report_type == \"summary\":\n",
        "            query[\"fields\"] = [\"id\", \"name\", \"status\", \"date_created\", \"assigned_user_id\"]\n",
        "        else:\n",
        "            query[\"fields\"] = [\"id\", \"name\", \"status\", \"date_created\", \"assigned_user_id\",\n",
        "                              \"email\", \"phone\", \"source\", \"description\"]\n",
        "\n",
        "        return query\n",
        "\n",
        "    def _generate_report(self, data: List[Dict], report_type: str) -> Dict:\n",
        "        \"\"\"Generate a structured report from the data\"\"\"\n",
        "        report = {\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"total_records\": len(data)\n",
        "        }\n",
        "\n",
        "        # Generate metrics based on report type\n",
        "        if report_type == \"summary\" or report_type == \"leads\":\n",
        "            # Group leads by status\n",
        "            status_counts = {}\n",
        "            for record in data:\n",
        "                status = record.get(\"status\", \"Unknown\")\n",
        "                status_counts[status] = status_counts.get(status, 0) + 1\n",
        "            report[\"status_distribution\"] = status_counts\n",
        "\n",
        "            # Calculate daily new leads\n",
        "            date_counts = {}\n",
        "            for record in data:\n",
        "                date_str = record.get(\"date_created\", \"\").split(\"T\")[0]\n",
        "                if date_str:\n",
        "                    date_counts[date_str] = date_counts.get(date_str, 0) + 1\n",
        "            report[\"daily_new_leads\"] = date_counts\n",
        "\n",
        "        elif report_type == \"opportunities\":\n",
        "            # Calculate total and average opportunity value\n",
        "            total_value = 0\n",
        "            for record in data:\n",
        "                value = float(record.get(\"amount\", 0))\n",
        "                total_value += value\n",
        "            report[\"total_opportunity_value\"] = total_value\n",
        "            report[\"average_opportunity_value\"] = total_value / len(data) if data else 0\n",
        "\n",
        "            # Calculate win rate\n",
        "            won_count = sum(1 for record in data if record.get(\"status\") == \"Won\")\n",
        "            report[\"win_rate\"] = won_count / len(data) if data else 0\n",
        "\n",
        "        # Add raw data for detailed reports\n",
        "        if report_type != \"summary\":\n",
        "            report[\"records\"] = data\n",
        "\n",
        "        return report\n",
        "\n",
        "# Main application entry point\n",
        "def main():\n",
        "    # Initialize workflow manager\n",
        "    workflow_manager = WorkflowManager()\n",
        "\n",
        "    # Example usage\n",
        "    user_input = \"Sync data from Odoo to Zoho\"\n",
        "    logger.info(f\"Processing workflow: {user_input}\")\n",
        "    result = workflow_manager.process_workflow(user_input)\n",
        "\n",
        "    if result.success:\n",
        "        logger.info(\"Workflow completed successfully\")\n",
        "        for message in result.messages:\n",
        "            print(message)\n",
        "        print(f\"Execution time: {result.execution_time:.2f}s\")\n",
        "    else:\n",
        "        logger.error(\"Workflow failed\")\n",
        "        for message in result.messages:\n",
        "            print(message)\n",
        "        print(f\"Error details: {result.error_details}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ga75vwrE_qEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oY7ru77l_qGw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}